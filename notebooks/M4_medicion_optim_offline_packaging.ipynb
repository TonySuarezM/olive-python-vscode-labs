{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0b6e95",
   "metadata": {},
   "source": [
    "# M4 — Medición, optimización offline y packaging (ORT + Olive)\n",
    "\n",
    "## Objetivo\n",
    "1. Medir **tamaño** y **latencia aproximada** de un modelo con ONNX Runtime (ORT).\n",
    "2. Usar **SessionOptions** para:\n",
    "   - elegir nivel de optimización de grafo\n",
    "   - guardar el **modelo optimizado offline** a disco\n",
    "   - generar un **perfil de latencia** (JSON)\n",
    "3. (Opcional) Generar un **ZIP de artefactos** con Olive (`packaging_config`).\n",
    "\n",
    "> Nota: el micro-benchmark de latencia aquí es orientativo (para comparar antes/después en tu máquina).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab8754",
   "metadata": {},
   "source": [
    "## Prerrequisitos (VS Code + Kernel)\n",
    "Asegúrate de que el notebook usa el **kernel** del entorno `.venv` del proyecto (en VS Code: selector de kernel arriba a la derecha).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ba50af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "ONNX Runtime: 1.23.2\n",
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "--- pip show olive-ai ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['g:\\\\source\\\\VisualCode\\\\repos\\\\olive-python-vscode-labs\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'show', 'olive-ai'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 1 — Comprobar que estamos en el Python del venv\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"ONNX Runtime:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "print(\"\\n--- pip show olive-ai ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32bb1ca",
   "metadata": {},
   "source": [
    "## 1) Seleccionar un modelo de trabajo\n",
    "\n",
    "Usaremos, si existe:\n",
    "- `models/linear_fp32.onnx` (del M3)\n",
    "- `outputs/m3_linear_int8/**.onnx` (si ya cuantizaste)\n",
    "\n",
    "Si no existen, creamos un modelo lineal FP32 mínimo (`Y = X·W + b`) en `models/linear_fp32.onnx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562dcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\models\\linear_fp32.onnx\n",
      "INT8 model: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\\model.onnx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "root = Path.cwd()\n",
    "# Si estás dentro de notebooks/, sube un nivel\n",
    "if not (root / \"models\").exists() and (root.parent / \"models\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "models_dir = root / \"models\"\n",
    "outputs_dir = root / \"outputs\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fp32_model_path = models_dir / \"linear_fp32.onnx\"\n",
    "\n",
    "if not fp32_model_path.exists():\n",
    "    in_features = 4\n",
    "    out_features = 3\n",
    "\n",
    "    X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [\"batch\", in_features])\n",
    "    Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [\"batch\", out_features])\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    W_val = rng.standard_normal((in_features, out_features), dtype=np.float32)\n",
    "    b_val = rng.standard_normal((out_features,), dtype=np.float32)\n",
    "\n",
    "    W = numpy_helper.from_array(W_val, name=\"W\")\n",
    "    b = numpy_helper.from_array(b_val, name=\"b\")\n",
    "\n",
    "    matmul = helper.make_node(\"MatMul\", inputs=[\"X\", \"W\"], outputs=[\"Z\"])\n",
    "    add = helper.make_node(\"Add\", inputs=[\"Z\", \"b\"], outputs=[\"Y\"])\n",
    "\n",
    "    graph = helper.make_graph([matmul, add], \"linear\", [X], [Y], initializer=[W, b])\n",
    "\n",
    "    opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "    model = helper.make_model(graph, producer_name=\"m4-lab\", opset_imports=opset)\n",
    "    model.ir_version = 11\n",
    "\n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save_model(model, str(fp32_model_path))\n",
    "\n",
    "print(\"FP32 model:\", fp32_model_path)\n",
    "\n",
    "# Buscar un INT8 cuantizado (si existe)\n",
    "int8_candidates = sorted((outputs_dir / \"m3_linear_int8\").rglob(\"*.onnx\")) if (outputs_dir / \"m3_linear_int8\").exists() else []\n",
    "int8_model_path = int8_candidates[0] if int8_candidates else None\n",
    "\n",
    "print(\"INT8 model:\", int8_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be95ba",
   "metadata": {},
   "source": [
    "## 2) Micro-benchmark (latencia) + tamaño\n",
    "\n",
    "Medimos:\n",
    "- tamaño del archivo en bytes\n",
    "- tiempo medio por inferencia (ms) con un input fijo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15344c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32: linear_fp32.onnx | size: 198 bytes | avg: 0.003832000002148561 ms | y: [[ 5.2784495  1.9592975 -5.466373 ]]\n",
      "INT8: model.onnx | size: 1180 bytes | avg: 0.004423333333155218 ms | y: [[ 5.2475195  1.9376892 -5.4539256]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "x = np.array([[1.0, 2.0, 3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "def benchmark(model_path: Path, sess_options=None, iters: int = 300, warmup: int = 30):\n",
    "    sess = ort.InferenceSession(str(model_path), sess_options=sess_options, providers=[\"CPUExecutionProvider\"])\n",
    "    inp = sess.get_inputs()[0].name\n",
    "    out = sess.get_outputs()[0].name\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        sess.run([out], {inp: x})\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        y = sess.run([out], {inp: x})[0]\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    avg_ms = (t1 - t0) * 1000 / iters\n",
    "    size = model_path.stat().st_size\n",
    "    return y, avg_ms, size\n",
    "\n",
    "y_fp32, ms_fp32, sz_fp32 = benchmark(fp32_model_path)\n",
    "print(\"FP32:\", fp32_model_path.name, \"| size:\", sz_fp32, \"bytes | avg:\", ms_fp32, \"ms | y:\", y_fp32)\n",
    "\n",
    "if int8_model_path:\n",
    "    y_int8, ms_int8, sz_int8 = benchmark(int8_model_path)\n",
    "    print(\"INT8:\", int8_model_path.name, \"| size:\", sz_int8, \"bytes | avg:\", ms_int8, \"ms | y:\", y_int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41f573",
   "metadata": {},
   "source": [
    "## 3) Optimización de grafo (online) y export optimizado (offline)\n",
    "\n",
    "ORT permite:\n",
    "- elegir el nivel de optimización de grafo (`graph_optimization_level`)\n",
    "- guardar el modelo resultante tras optimizaciones en disco (`optimized_model_filepath`)\n",
    "\n",
    "Aquí generamos un modelo optimizado offline y luego lo medimos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65d9d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized model: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m4_optimized\\linear_optimized.onnx | exists: True | bytes: 454\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "optimized_dir = outputs_dir / \"m4_optimized\"\n",
    "optimized_dir.mkdir(exist_ok=True)\n",
    "\n",
    "optimized_model_path = optimized_dir / \"linear_optimized.onnx\"\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "# Nivel de optimización: EXTENDED (puedes probar ORT_ENABLE_ALL)\n",
    "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "# Export del grafo optimizado\n",
    "so.optimized_model_filepath = str(optimized_model_path)\n",
    "\n",
    "# Crear la sesión dispara la optimización y guarda el modelo optimizado\n",
    "_ = ort.InferenceSession(str(fp32_model_path), sess_options=so, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Saved optimized model:\", optimized_model_path, \"| exists:\", optimized_model_path.exists(), \"| bytes:\", optimized_model_path.stat().st_size if optimized_model_path.exists() else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f5e2c",
   "metadata": {},
   "source": [
    "## 4) Medir el modelo optimizado offline\n",
    "\n",
    "Cuando cargas el modelo ya optimizado, puedes desactivar optimizaciones para reducir trabajo de inicialización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f6ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPT_OFFLINE: linear_optimized.onnx | size: 454 bytes | avg: 0.003671999999520873 ms | y: [[ 5.2784495  1.9592975 -5.466373 ]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "so_no_opt = ort.SessionOptions()\n",
    "so_no_opt.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "\n",
    "y_opt, ms_opt, sz_opt = benchmark(optimized_model_path, sess_options=so_no_opt)\n",
    "print(\"OPT_OFFLINE:\", optimized_model_path.name, \"| size:\", sz_opt, \"bytes | avg:\", ms_opt, \"ms | y:\", y_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bb1c0",
   "metadata": {},
   "source": [
    "## 5) Profiling de latencia (JSON)\n",
    "\n",
    "ORT puede generar un archivo JSON de profiling:\n",
    "- `SessionOptions.enable_profiling = True`\n",
    "- al final llamas a `session.end_profiling()` para obtener el nombre del archivo generado\n",
    "\n",
    "En esta celda generamos un perfil y hacemos un resumen simple por `name` (duración total).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65546975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile file: G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m4_profiles\\ort_profile_2026-01-12_10-35-58.json | exists: True\n",
      "\n",
      "Top 15 eventos por dur total (unidad típica: microsegundos):\n",
      "- model_run: 924\n",
      "- SequentialExecutor::Execute: 813\n",
      "- /MatMulAddFusion_kernel_time: 688\n",
      "- session_initialization: 315\n",
      "- model_loading_uri: 166\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "profile_dir = outputs_dir / \"m4_profiles\"\n",
    "profile_dir.mkdir(exist_ok=True)\n",
    "\n",
    "so_prof = ort.SessionOptions()\n",
    "so_prof.enable_profiling = True\n",
    "so_prof.profile_file_prefix = str(profile_dir / \"ort_profile\")\n",
    "\n",
    "sess = ort.InferenceSession(str(fp32_model_path), sess_options=so_prof, providers=[\"CPUExecutionProvider\"])\n",
    "inp = sess.get_inputs()[0].name\n",
    "out = sess.get_outputs()[0].name\n",
    "\n",
    "# Ejecutar algunas inferencias\n",
    "for _ in range(50):\n",
    "    sess.run([out], {inp: x})\n",
    "\n",
    "prof_file = sess.end_profiling()\n",
    "prof_path = Path(prof_file)\n",
    "print(\"Profile file:\", prof_path.resolve(), \"| exists:\", prof_path.exists())\n",
    "\n",
    "data = json.loads(prof_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "dur_by_name = defaultdict(int)\n",
    "for ev in data:\n",
    "    name = ev.get(\"name\")\n",
    "    dur = ev.get(\"dur\")\n",
    "    if name is not None and dur is not None:\n",
    "        dur_by_name[name] += dur\n",
    "\n",
    "top = sorted(dur_by_name.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(\"\\nTop 15 eventos por dur total (unidad típica: microsegundos):\")\n",
    "for name, dur in top:\n",
    "    print(f\"- {name}: {dur}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fe5a5",
   "metadata": {},
   "source": [
    "## 6) (Opcional) Packaging con Olive (Zipfile)\n",
    "\n",
    "Olive puede empaquetar artefactos en un ZIP cuando añades `packaging_config` en el `engine` del run-config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2882c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m4_pack_run_config.json\n",
      "Running: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe -m olive run --config g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m4_pack_run_config.json\n",
      "returncode: 0\n",
      "---- stderr (tail) ----\n",
      "\n",
      "ZIPs: []\n"
     ]
    }
   ],
   "source": [
    "# Celda opcional — generar ZIP con Olive a partir de un workflow simple (sin depender de passes problemáticos)\n",
    "# Requiere que 'olive' se ejecute con el Python del kernel: sys.executable -m olive ...\n",
    "\n",
    "import sys, subprocess, json\n",
    "from pathlib import Path\n",
    "\n",
    "package_out_dir = outputs_dir / \"m4_olive_package\"\n",
    "package_cache = outputs_dir / \"m4_olive_cache\"\n",
    "cfg_path = outputs_dir / \"m4_pack_run_config.json\"\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m4_pack_fp32_only\",\n",
    "    \"input_model\": {\n",
    "        # En algunas versiones el registry key del handler ONNX es \"ONNXModel\".\n",
    "        \"type\": \"ONNXModel\",\n",
    "        \"config\": {\"model_path\": str(fp32_model_path)},\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\"accelerators\": [{\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}]},\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {},\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(package_cache),\n",
    "        \"output_dir\": str(package_out_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "        \"packaging_config\": {\"type\": \"Zipfile\", \"name\": \"M4_OutputModels\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "cfg_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", cfg_path)\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(cfg_path)]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "completed = subprocess.run(cmd, text=True, capture_output=True)\n",
    "print(\"returncode:\", completed.returncode)\n",
    "print(\"---- stderr (tail) ----\")\n",
    "print(completed.stderr[-2000:])\n",
    "\n",
    "zips = sorted(package_out_dir.rglob(\"*.zip\")) if package_out_dir.exists() else []\n",
    "print(\"ZIPs:\", zips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5091c5",
   "metadata": {},
   "source": [
    "## Verificación (checklist)\n",
    "- Has medido `FP32` (y `INT8` si existe) con tamaño y latencia.\n",
    "- Existe `outputs/m4_optimized/linear_optimized.onnx`.\n",
    "- Existe un perfil JSON en `outputs/m4_profiles/`.\n",
    "- (Opcional) Se genera un ZIP en `outputs/m4_olive_package/` con `packaging_config`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
