{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9fc14c",
   "metadata": {},
   "source": [
    "# M2 — Primer pipeline Olive end-to-end (ONNX → optimizaciones → validación)\n",
    "\n",
    "Este notebook continúa el curso **Curso Olive + Python (VS Code)**.\n",
    "\n",
    "## Objetivo\n",
    "1. Tomar un modelo **ONNX** (el `models/add_const.onnx` del M1).\n",
    "2. Ejecutar un **workflow de Olive** con un archivo de configuración (`.json`) usando `python -m olive run`.\n",
    "3. Revisar artefactos en `outputs/` y **validar** que el modelo optimizado mantiene el resultado.\n",
    "\n",
    "## Prerrequisitos\n",
    "- VS Code con notebooks Jupyter.\n",
    "- Entorno `.venv` activado.\n",
    "- Paquetes instalados en el venv: `olive-ai`, `onnxruntime`, `onnx`, `numpy`.\n",
    "\n",
    "Estructura recomendada del repo:\n",
    "```\n",
    "notebooks/\n",
    "models/\n",
    "outputs/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Comprobación de entorno (versiones)\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"ONNX Runtime:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "# (Obligatorio en el curso) validar versión de Olive y Python\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd57a1",
   "metadata": {},
   "source": [
    "## Explicación breve\n",
    "\n",
    "Olive ejecuta una **secuencia de passes** (transformaciones/optimizaciones) definida en un archivo JSON/YAML.\n",
    "\n",
    "En este M2 vamos a:\n",
    "- Configurar un **sistema local** (CPU).\n",
    "- Aplicar dos passes sencillos sobre un modelo ONNX:\n",
    "  - `OnnxOpVersionConversion` (para fijar opset objetivo).\n",
    "  - `OnnxModelOptimizer` (optimizaciones/fusiones estándar sobre el grafo).\n",
    "- Guardar el resultado en `outputs/m2_add_const/` y comprobar inferencia con ONNX Runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08c3f8",
   "metadata": {},
   "source": [
    "# Práctica en Notebook\n",
    "\n",
    "## 1) Asegurar el modelo de entrada (M1)\n",
    "\n",
    "Si ya tienes `models/add_const.onnx`, esta celda no modifica nada.\n",
    "Si no existe, lo crea (modelo: `Y = X + 1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "model_path = Path(\"../models\") / \"add_const.onnx\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    # 1) Definir IO del modelo\n",
    "    X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [None])  # vector 1D de tamaño variable\n",
    "    Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None])\n",
    "\n",
    "    # 2) Constante C (float32)\n",
    "    C_value = np.array([1.0], dtype=np.float32)\n",
    "    C = numpy_helper.from_array(C_value, name=\"C\")\n",
    "\n",
    "    # 3) Nodo: Add(X, C) -> Y\n",
    "    node = helper.make_node(\"Add\", inputs=[\"X\", \"C\"], outputs=[\"Y\"])\n",
    "\n",
    "    # 4) Grafo + Modelo\n",
    "    graph = helper.make_graph(nodes=[node], name=\"add-const\", inputs=[X], outputs=[Y], initializer=[C])\n",
    "\n",
    "    # Crear el modelo con opset 11 e IR 11 (compatibilidad amplia con runtimes)\n",
    "    opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "    model = helper.make_model(graph, producer_name=\"m2-lab\", opset_imports=opset)\n",
    "    model.ir_version = 11\n",
    "\n",
    "    # 5) Validar y guardar\n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save_model(model, str(model_path))\n",
    "\n",
    "print(\"Input model:\", model_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcf78f",
   "metadata": {},
   "source": [
    "## 2) Crear el config JSON de Olive\n",
    "\n",
    "Vamos a escribir un archivo `outputs/m2_run_config.json` y luego ejecutar:\n",
    "\n",
    "```\n",
    "python -m olive run --config outputs/m2_run_config.json\n",
    "```\n",
    "\n",
    "Nota: usamos `python -m olive` para no depender de que `olive` esté en el PATH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar raíz del proyecto (si estás en notebooks/, sube 1)\n",
    "root = Path.cwd()\n",
    "if not (root / \"models\" / \"add_const.onnx\").exists() and (root.parent / \"models\" / \"add_const.onnx\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "model_path = root / \"models\" / \"add_const.onnx\"\n",
    "run_config_path = root / \"outputs\" / \"m2_run_config.json\"\n",
    "output_dir = root / \"outputs\" / \"m2_add_const\"\n",
    "cache_dir = root / \"outputs\" / \"m2_cache\"\n",
    "\n",
    "(root / \"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m2_add_const_e2e\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"ONNXModel\",  # Nota: en versiones recientes de Olive es ONNXModel, no ONNXModelHandler\n",
    "        \"config\": {\n",
    "            \"model_path\": str(model_path),\n",
    "        },\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\n",
    "                        \"device\": \"cpu\",\n",
    "                        \"execution_providers\": [\"CPUExecutionProvider\"],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"opset_to_11\": {\n",
    "            \"type\": \"OnnxOpVersionConversion\",\n",
    "            \"config\": {\"target_opset\": 11},\n",
    "        },\n",
    "        \"onnx_optimize\": {\n",
    "            \"type\": \"OnnxPeepholeOptimizer\",  # Nota: OnnxModelOptimizer ya no existe, usar OnnxPeepholeOptimizer\n",
    "            \"config\": {},\n",
    "        },\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", run_config_path)\n",
    "print(\"Model:\", model_path)\n",
    "print(\"Output:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd2320",
   "metadata": {},
   "source": [
    "## 5) Validación: inferencia con ONNX Runtime (baseline vs optimizado)\n",
    "\n",
    "- Cargamos el modelo original (`models/add_const.onnx`)\n",
    "- Cargamos un modelo ONNX generado por Olive\n",
    "- Ejecutamos con el mismo input y comprobamos que el resultado coincide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "\n",
    "baseline = Path(\"../models\") / \"add_const.onnx\"\n",
    "\n",
    "# Buscar archivos .onnx generados por Olive\n",
    "onnx_files = list(Path(\"../outputs/m2_add_const\").rglob(\"*.onnx\"))\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(\"No se encontraron archivos .onnx en outputs/m2_add_const. Revisa el log de Olive.\")\n",
    "\n",
    "# Elegimos el primer .onnx encontrado. Si Olive genera varios, puedes cambiar este criterio.\n",
    "optimized = onnx_files[0]\n",
    "\n",
    "def run_ort(model_path: Path, x: np.ndarray) -> np.ndarray:\n",
    "    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "    y = sess.run([output_name], {input_name: x})[0]\n",
    "    return y\n",
    "\n",
    "x = np.array([10, 20, 30], dtype=np.float32)\n",
    "y_base = run_ort(baseline, x)\n",
    "y_opt = run_ort(optimized, x)\n",
    "\n",
    "print(\"baseline:\", baseline.resolve())\n",
    "print(\"optimized:\", optimized.resolve())\n",
    "print(\"x:\", x)\n",
    "print(\"y_base:\", y_base)\n",
    "print(\"y_opt :\", y_opt)\n",
    "print(\"All close:\", np.allclose(y_base, y_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05956d81",
   "metadata": {},
   "source": [
    "## Verificación (qué debe salir)\n",
    "\n",
    "- `returncode: 0` en la celda de ejecución de Olive.\n",
    "- En `outputs/m2_add_const/` debe aparecer al menos un `.onnx`.\n",
    "- En la validación final: `All close: True`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db54d4b",
   "metadata": {},
   "source": [
    "## Errores comunes y diagnóstico\n",
    "\n",
    "1) **No aparece ningún `.onnx` en outputs/**\n",
    "   - Revisar el `stderr`/`stdout` de la celda 3.\n",
    "   - Confirmar que el `model_path` existe y apunta al ONNX correcto.\n",
    "\n",
    "2) **El pass no existe / nombre incorrecto**\n",
    "   - Pasa cuando tu versión instalada difiere. En ese caso, mira la ayuda / docs de tu versión, o pega el error.\n",
    "\n",
    "3) **Problemas de proveedores ORT**\n",
    "   - Aquí forzamos `CPUExecutionProvider` para hacerlo reproducible.\n",
    "\n",
    "## Siguiente paso (M3)\n",
    "En M3 vamos a introducir **cuantización** (por ejemplo, `OnnxDynamicQuantization`) y hablaremos de **calidad vs rendimiento**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
