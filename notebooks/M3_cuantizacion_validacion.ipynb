{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04bd224d",
   "metadata": {},
   "source": [
    "# M3 — Cuantización y validación (calidad vs rendimiento) con Olive + ONNX Runtime\n",
    "\n",
    "## Objetivo\n",
    "1. Crear un modelo ONNX pequeño **con pesos** (para que la cuantización tenga efecto).\n",
    "2. Medir **tamaño** y una **latencia aproximada** (micro‑benchmark) en ORT.\n",
    "3. Ejecutar un workflow de **Olive** que aplique cuantización ONNX (INT8) y genere un modelo nuevo.\n",
    "4. Validar que la salida del modelo cuantizado sigue siendo “equivalente” (dentro de tolerancia).\n",
    "\n",
    "## Prerrequisitos\n",
    "- Ejecutar el notebook con el **kernel** de tu `.venv`.\n",
    "- Paquetes: `olive-ai`, `onnxruntime`, `onnx`, `numpy`.\n",
    "\n",
    "## Referencias oficiales (para el módulo)\n",
    "- CLI: `olive run --config ...` y `python -m olive` si `olive` no está en PATH. (Quick Tour)  \n",
    "- Config `input_model`: `{ \"type\": \"...ModelHandler\", \"config\": {...} }` y soporta `ONNXModelHandler`. (Options)  \n",
    "- Cuantización ONNX: `OnnxQuantization` y también `OnnxDynamicQuantization` / `OnnxStaticQuantization`. (Quantization)  \n",
    "- Pass `OnnxDynamicQuantization` incluye `quant_mode` (default `dynamic`) y `weight_type` (default `QInt8`). (Passes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Chequeo de entorno (siempre)\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"ONNX Runtime:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "print(\"\\n--- pip show olive-ai ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6cad7",
   "metadata": {},
   "source": [
    "## 1) Crear un modelo ONNX con pesos (Linear: Y = X·W + b)\n",
    "\n",
    "Creamos un ONNX con `MatMul` + `Add` y guardamos en `models/linear_fp32.onnx`.\n",
    "\n",
    "- Input: `X` shape `[batch, in_features]` (batch dinámico)\n",
    "- Pesos: `W` shape `[in_features, out_features]`\n",
    "- Bias: `b` shape `[out_features]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "in_features = 4\n",
    "out_features = 3\n",
    "\n",
    "# IO\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [\"batch\", in_features])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [\"batch\", out_features])\n",
    "\n",
    "# Pesos y bias (float32)\n",
    "rng = np.random.default_rng(0)\n",
    "W_val = rng.standard_normal((in_features, out_features), dtype=np.float32)\n",
    "b_val = rng.standard_normal((out_features,), dtype=np.float32)\n",
    "\n",
    "W = numpy_helper.from_array(W_val, name=\"W\")\n",
    "b = numpy_helper.from_array(b_val, name=\"b\")\n",
    "\n",
    "# Nodos: MatMul(X, W) -> Z ; Add(Z, b) -> Y\n",
    "matmul = helper.make_node(\"MatMul\", inputs=[\"X\", \"W\"], outputs=[\"Z\"])\n",
    "add = helper.make_node(\"Add\", inputs=[\"Z\", \"b\"], outputs=[\"Y\"])\n",
    "\n",
    "graph = helper.make_graph(\n",
    "    nodes=[matmul, add],\n",
    "    name=\"linear\",\n",
    "    inputs=[X],\n",
    "    outputs=[Y],\n",
    "    initializer=[W, b],\n",
    ")\n",
    "\n",
    "# Para compatibilidad amplia: opset 11 e IR 11\n",
    "opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "model = helper.make_model(graph, producer_name=\"m3-lab\", opset_imports=opset)\n",
    "model.ir_version = 11\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "fp32_model_path = Path(\"../models\") / \"linear_fp32.onnx\"\n",
    "onnx.save_model(model, str(fp32_model_path))\n",
    "\n",
    "fp32_model_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5be5cd",
   "metadata": {},
   "source": [
    "## 2) Baseline con ONNX Runtime: validar y medir (aprox)\n",
    "\n",
    "Medimos:\n",
    "- tamaño del archivo (`bytes`)\n",
    "- tiempo medio por inferencia (simple micro‑benchmark)\n",
    "\n",
    "> Nota: esto es una medida aproximada para comparar “antes vs después”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def run_and_time(model_path: Path, x: np.ndarray, iters: int = 200, warmup: int = 20):\n",
    "    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "    inp = sess.get_inputs()[0].name\n",
    "    out = sess.get_outputs()[0].name\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        sess.run([out], {inp: x})\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        y = sess.run([out], {inp: x})[0]\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    avg_ms = (t1 - t0) * 1000 / iters\n",
    "    return y, avg_ms\n",
    "\n",
    "x = np.array([[1.0, 2.0, 3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "y_fp32, fp32_ms = run_and_time(fp32_model_path, x)\n",
    "fp32_size = fp32_model_path.stat().st_size\n",
    "\n",
    "print(\"FP32 model:\", fp32_model_path)\n",
    "print(\"Size (bytes):\", fp32_size)\n",
    "print(\"Avg latency (ms):\", fp32_ms)\n",
    "print(\"y_fp32:\", y_fp32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163188a3",
   "metadata": {},
   "source": [
    "## 3) Workflow Olive: cuantización ONNX (dinámica, INT8)\n",
    "\n",
    "En este laboratorio usamos `OnnxDynamicQuantization` (modo `dynamic`) para evitar calibración/datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolver raíz del repo (si el notebook está en notebooks/, sube un nivel)\n",
    "root = Path.cwd()\n",
    "if not (root / \"models\" / \"linear_fp32.onnx\").exists() and (root.parent / \"models\" / \"linear_fp32.onnx\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "model_path = root / \"models\" / \"linear_fp32.onnx\"\n",
    "output_dir = root / \"outputs\" / \"m3_linear_int8\"\n",
    "cache_dir = root / \"outputs\" / \"m3_cache\"\n",
    "config_path = root / \"outputs\" / \"m3_run_config.json\"\n",
    "\n",
    "(root / \"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m3_linear_quant_dynamic\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"ONNXModel\",\n",
    "        \"config\": {\"model_path\": str(model_path)},\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"quant_int8\": {\n",
    "            \"type\": \"OnnxDynamicQuantization\",\n",
    "            \"config\": {\n",
    "            \"quant_mode\": \"dynamic\",\n",
    "            \"weight_type\": \"QInt8\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", config_path)\n",
    "print(\"Output dir:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772405c",
   "metadata": {},
   "source": [
    "## 4) Ejecutar Olive\n",
    "\n",
    "Usamos `sys.executable -m olive ...` para asegurar que se ejecuta con el Python del kernel (tu `.venv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ace703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(config_path)]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "completed = subprocess.run(cmd, text=True, capture_output=True)\n",
    "print(\"returncode:\", completed.returncode)\n",
    "print(\"---- stdout ----\")\n",
    "print(completed.stdout[-4000:])  # último tramo para no saturar\n",
    "print(\"---- stderr ----\")\n",
    "print(completed.stderr[-4000:])\n",
    "\n",
    "if completed.returncode != 0:\n",
    "    raise RuntimeError(\"Olive falló. Revisa stdout/stderr arriba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d3164",
   "metadata": {},
   "source": [
    "## 5) Localizar el modelo cuantizado generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ab56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out = output_dir\n",
    "onnx_files = sorted(out.rglob(\"*.onnx\"))\n",
    "print(\"ONNX files:\", len(onnx_files))\n",
    "for p in onnx_files:\n",
    "    print(\"-\", p.relative_to(out))\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(f\"No encuentro .onnx bajo {out}. Revisa logs de Olive.\")\n",
    "\n",
    "int8_model_path = onnx_files[0]\n",
    "print(\"\\nSelected:\", int8_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b39113",
   "metadata": {},
   "source": [
    "## 6) Validación numérica + comparación de tamaño/latencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce401d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_int8, int8_ms = run_and_time(int8_model_path, x)\n",
    "int8_size = int8_model_path.stat().st_size\n",
    "\n",
    "print(\"INT8 model:\", int8_model_path)\n",
    "print(\"Size (bytes):\", int8_size)\n",
    "print(\"Avg latency (ms):\", int8_ms)\n",
    "print(\"y_int8:\", y_int8)\n",
    "\n",
    "print(\"\\nAllclose(fp32, int8):\", np.allclose(y_fp32, y_int8, rtol=1e-02, atol=1e-03))\n",
    "print(\"Max abs diff:\", float(np.max(np.abs(y_fp32 - y_int8))))\n",
    "print(\"Size reduction (%):\", (1 - int8_size / fp32_size) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c4fe5",
   "metadata": {},
   "source": [
    "## Verificación (checklist)\n",
    "- `outputs/m3_linear_int8/` contiene al menos un `.onnx`.\n",
    "- El modelo INT8 carga con ORT (sin errores).\n",
    "- `np.allclose(...)` es `True` (o el error es pequeño).\n",
    "- (Opcional) el tamaño del archivo baja y/o la latencia mejora.\n",
    "\n",
    "## Siguiente paso (M4)\n",
    "Medición más seria (latencia/throughput), empaquetado y preparación para hardware objetivo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
