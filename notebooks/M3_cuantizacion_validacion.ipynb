{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04bd224d",
   "metadata": {},
   "source": [
    "# M3 — Cuantización y validación (calidad vs rendimiento) con Olive + ONNX Runtime\n",
    "\n",
    "## Objetivo\n",
    "1. Crear un modelo ONNX pequeño **con pesos** (para que la cuantización tenga efecto).\n",
    "2. Medir **tamaño** y una **latencia aproximada** (micro‑benchmark) en ORT.\n",
    "3. Ejecutar un workflow de **Olive** que aplique cuantización ONNX (INT8) y genere un modelo nuevo.\n",
    "4. Validar que la salida del modelo cuantizado sigue siendo “equivalente” (dentro de tolerancia).\n",
    "\n",
    "## Prerrequisitos\n",
    "- Ejecutar el notebook con el **kernel** de tu `.venv`.\n",
    "- Paquetes: `olive-ai`, `onnxruntime`, `onnx`, `numpy`.\n",
    "\n",
    "## Referencias oficiales (para el módulo)\n",
    "- CLI: `olive run --config ...` y `python -m olive` si `olive` no está en PATH. (Quick Tour)  \n",
    "- Config `input_model`: `{ \"type\": \"...ModelHandler\", \"config\": {...} }` y soporta `ONNXModelHandler`. (Options)  \n",
    "- Cuantización ONNX: `OnnxQuantization` y también `OnnxDynamicQuantization` / `OnnxStaticQuantization`. (Quantization)  \n",
    "- Pass `OnnxDynamicQuantization` incluye `quant_mode` (default `dynamic`) y `weight_type` (default `QInt8`). (Passes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6f6313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "ONNX Runtime: 1.23.2\n",
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "\n",
      "--- pip show olive-ai ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['g:\\\\source\\\\VisualCode\\\\repos\\\\olive-python-vscode-labs\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'show', 'olive-ai'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Celda 1 — Chequeo de entorno (siempre)\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"ONNX Runtime:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "print(\"\\n--- pip show olive-ai ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec6cad7",
   "metadata": {},
   "source": [
    "## 1) Crear un modelo ONNX con pesos (Linear: Y = X·W + b)\n",
    "\n",
    "Creamos un ONNX con `MatMul` + `Add` y guardamos en `models/linear_fp32.onnx`.\n",
    "\n",
    "- Input: `X` shape `[batch, in_features]` (batch dinámico)\n",
    "- Pesos: `W` shape `[in_features, out_features]`\n",
    "- Bias: `b` shape `[out_features]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c014ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../models/linear_fp32.onnx')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "in_features = 4\n",
    "out_features = 3\n",
    "\n",
    "# IO\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [\"batch\", in_features])\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [\"batch\", out_features])\n",
    "\n",
    "# Pesos y bias (float32)\n",
    "rng = np.random.default_rng(0)\n",
    "W_val = rng.standard_normal((in_features, out_features), dtype=np.float32)\n",
    "b_val = rng.standard_normal((out_features,), dtype=np.float32)\n",
    "\n",
    "W = numpy_helper.from_array(W_val, name=\"W\")\n",
    "b = numpy_helper.from_array(b_val, name=\"b\")\n",
    "\n",
    "# Nodos: MatMul(X, W) -> Z ; Add(Z, b) -> Y\n",
    "matmul = helper.make_node(\"MatMul\", inputs=[\"X\", \"W\"], outputs=[\"Z\"])\n",
    "add = helper.make_node(\"Add\", inputs=[\"Z\", \"b\"], outputs=[\"Y\"])\n",
    "\n",
    "graph = helper.make_graph(\n",
    "    nodes=[matmul, add],\n",
    "    name=\"linear\",\n",
    "    inputs=[X],\n",
    "    outputs=[Y],\n",
    "    initializer=[W, b],\n",
    ")\n",
    "\n",
    "# Para compatibilidad amplia: opset 11 e IR 11\n",
    "opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "model = helper.make_model(graph, producer_name=\"m3-lab\", opset_imports=opset)\n",
    "model.ir_version = 11\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "fp32_model_path = Path(\"../models\") / \"linear_fp32.onnx\"\n",
    "onnx.save_model(model, str(fp32_model_path))\n",
    "\n",
    "fp32_model_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5be5cd",
   "metadata": {},
   "source": [
    "## 2) Baseline con ONNX Runtime: validar y medir (aprox)\n",
    "\n",
    "Medimos:\n",
    "- tamaño del archivo (`bytes`)\n",
    "- tiempo medio por inferencia (simple micro‑benchmark)\n",
    "\n",
    "> Nota: esto es una medida aproximada para comparar “antes vs después”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4e3da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model: ..\\models\\linear_fp32.onnx\n",
      "Size (bytes): 198\n",
      "Avg latency (ms): 0.005691999999726249\n",
      "y_fp32: [[ 5.2784495  1.9592975 -5.466373 ]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def run_and_time(model_path: Path, x: np.ndarray, iters: int = 200, warmup: int = 20):\n",
    "    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "    inp = sess.get_inputs()[0].name\n",
    "    out = sess.get_outputs()[0].name\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(warmup):\n",
    "        sess.run([out], {inp: x})\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        y = sess.run([out], {inp: x})[0]\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    avg_ms = (t1 - t0) * 1000 / iters\n",
    "    return y, avg_ms\n",
    "\n",
    "x = np.array([[1.0, 2.0, 3.0, 4.0]], dtype=np.float32)\n",
    "\n",
    "y_fp32, fp32_ms = run_and_time(fp32_model_path, x)\n",
    "fp32_size = fp32_model_path.stat().st_size\n",
    "\n",
    "print(\"FP32 model:\", fp32_model_path)\n",
    "print(\"Size (bytes):\", fp32_size)\n",
    "print(\"Avg latency (ms):\", fp32_ms)\n",
    "print(\"y_fp32:\", y_fp32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163188a3",
   "metadata": {},
   "source": [
    "## 3) Workflow Olive: cuantización ONNX (dinámica, INT8)\n",
    "\n",
    "En este laboratorio usamos `OnnxDynamicQuantization` (modo `dynamic`) para evitar calibración/datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289c5a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_run_config.json\n",
      "Output dir: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolver raíz del repo (si el notebook está en notebooks/, sube un nivel)\n",
    "root = Path.cwd()\n",
    "if not (root / \"models\" / \"linear_fp32.onnx\").exists() and (root.parent / \"models\" / \"linear_fp32.onnx\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "model_path = root / \"models\" / \"linear_fp32.onnx\"\n",
    "output_dir = root / \"outputs\" / \"m3_linear_int8\"\n",
    "cache_dir = root / \"outputs\" / \"m3_cache\"\n",
    "config_path = root / \"outputs\" / \"m3_run_config.json\"\n",
    "\n",
    "(root / \"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m3_linear_quant_dynamic\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"ONNXModel\",\n",
    "        \"config\": {\"model_path\": str(model_path)},\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"quant_int8\": {\n",
    "            \"type\": \"OnnxDynamicQuantization\",\n",
    "            \"config\": {\n",
    "            \"quant_mode\": \"dynamic\",\n",
    "            \"weight_type\": \"QInt8\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", config_path)\n",
    "print(\"Output dir:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772405c",
   "metadata": {},
   "source": [
    "## 4) Ejecutar Olive\n",
    "\n",
    "Usamos `sys.executable -m olive ...` para asegurar que se ejecuta con el Python del kernel (tu `.venv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ace703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe -m olive run --config g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_run_config.json\n",
      "returncode: 0\n",
      "---- stdout ----\n",
      "rkflow m3_linear_quant_dynamic\n",
      "[2026-01-12 10:26:35,422] [INFO] [cache.py:138:__init__] Using cache directory: G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_cache\\m3_linear_quant_dynamic\n",
      "[2026-01-12 10:26:35,422] [DEBUG] [cache.py:274:cache_olive_config] Cached olive config to G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_cache\\m3_linear_quant_dynamic\\olive_config.json\n",
      "[2026-01-12 10:26:35,426] [DEBUG] [accelerator_creator.py:106:_fill_accelerators] The accelerator device and execution providers are specified, skipping deduce.\n",
      "[2026-01-12 10:26:35,427] [DEBUG] [accelerator_creator.py:143:_check_execution_providers] Supported execution providers for device cpu: [<ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>, <ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>]\n",
      "[2026-01-12 10:26:35,427] [DEBUG] [accelerator_creator.py:179:create_accelerator] Initial accelerators and execution providers: {'cpu': ['CPUExecutionProvider']}\n",
      "[2026-01-12 10:26:35,427] [INFO] [accelerator_creator.py:195:create_accelerator] Running workflow on accelerator spec: cpu-cpu\n",
      "[2026-01-12 10:26:35,427] [DEBUG] [cache.py:295:set_cache_env] Set OLIVE_CACHE_DIR: G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_cache\\m3_linear_quant_dynamic\n",
      "[2026-01-12 10:26:35,428] [INFO] [engine.py:206:run] Running Olive on accelerator: cpu-cpu\n",
      "[2026-01-12 10:26:35,428] [INFO] [engine.py:824:_create_system] Creating target system ...\n",
      "[2026-01-12 10:26:35,428] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2026-01-12 10:26:35,428] [INFO] [engine.py:827:_create_system] Target system created in 0.000189 seconds\n",
      "[2026-01-12 10:26:35,428] [INFO] [engine.py:830:_create_system] Creating host system ...\n",
      "[2026-01-12 10:26:35,428] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2026-01-12 10:26:35,428] [INFO] [engine.py:833:_create_system] Host system created in 0.000100 seconds\n",
      "[2026-01-12 10:26:35,430] [DEBUG] [engine.py:284:run_accelerator] Running Olive in no-search mode ...\n",
      "[2026-01-12 10:26:35,433] [WARNING] [config_utils.py:347:validate_config] Keys {'weight_type'} are not part of OnnxDynamicQuantizationConfig. Ignoring them.\n",
      "[2026-01-12 10:26:35,433] [DEBUG] [engine.py:326:_run_no_search] Running ['quant_int8'] with no search ...\n",
      "[2026-01-12 10:26:35,433] [INFO] [engine.py:656:_run_pass] Running pass quant_int8:onnxdynamicquantization\n",
      "[2026-01-12 10:26:35,468] [INFO] [quantization.py:430:_run_for_config] Preprocessing model for quantization\n",
      "[2026-01-12 10:26:35,510] [INFO] [engine.py:724:_run_pass] Pass quant_int8:onnxdynamicquantization finished in 0.076600 seconds\n",
      "[2026-01-12 10:26:35,510] [DEBUG] [cache.py:182:cache_model] Cached model a831fb32 to G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_cache\\m3_linear_quant_dynamic\\runs\\a831fb32\\model.json\n",
      "[2026-01-12 10:26:35,511] [DEBUG] [cache.py:224:cache_run] Cached run a831fb32 to G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_cache\\m3_linear_quant_dynamic\\runs\\a831fb32\\run.json\n",
      "[2026-01-12 10:26:35,511] [DEBUG] [engine.py:636:_run_passes] Signal: None, ['a831fb32']\n",
      "[2026-01-12 10:26:35,511] [INFO] [engine.py:294:run_accelerator] Save footprint to G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\\footprint.json.\n",
      "[2026-01-12 10:26:35,512] [DEBUG] [engine.py:296:run_accelerator] run_accelerator done\n",
      "[2026-01-12 10:26:35,512] [INFO] [engine.py:215:run] Run history for cpu-cpu:\n",
      "[2026-01-12 10:26:35,512] [INFO] [engine.py:472:_dump_run_history] Please install tabulate for better run history output\n",
      "[2026-01-12 10:26:35,512] [DEBUG] [engine.py:230:run] No packaging config provided, skip packaging artifacts\n",
      "[2026-01-12 10:26:35,512] [INFO] [cache.py:195:load_model] Loading model a831fb32 from cache.\n",
      "[2026-01-12 10:26:35,520] [INFO] [engine.py:234:run] Saved output model to G:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\n",
      "\n",
      "---- stderr ----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(config_path)]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "completed = subprocess.run(cmd, text=True, capture_output=True)\n",
    "print(\"returncode:\", completed.returncode)\n",
    "print(\"---- stdout ----\")\n",
    "print(completed.stdout[-4000:])  # último tramo para no saturar\n",
    "print(\"---- stderr ----\")\n",
    "print(completed.stderr[-4000:])\n",
    "\n",
    "if completed.returncode != 0:\n",
    "    raise RuntimeError(\"Olive falló. Revisa stdout/stderr arriba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d3164",
   "metadata": {},
   "source": [
    "## 5) Localizar el modelo cuantizado generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3ab56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX files: 1\n",
      "- model.onnx\n",
      "\n",
      "Selected: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\\model.onnx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out = output_dir\n",
    "onnx_files = sorted(out.rglob(\"*.onnx\"))\n",
    "print(\"ONNX files:\", len(onnx_files))\n",
    "for p in onnx_files:\n",
    "    print(\"-\", p.relative_to(out))\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(f\"No encuentro .onnx bajo {out}. Revisa logs de Olive.\")\n",
    "\n",
    "int8_model_path = onnx_files[0]\n",
    "print(\"\\nSelected:\", int8_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b39113",
   "metadata": {},
   "source": [
    "## 6) Validación numérica + comparación de tamaño/latencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce401d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 model: g:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m3_linear_int8\\model.onnx\n",
      "Size (bytes): 1180\n",
      "Avg latency (ms): 0.004530999999587948\n",
      "y_int8: [[ 5.2475195  1.9376892 -5.4539256]]\n",
      "\n",
      "Allclose(fp32, int8): False\n",
      "Max abs diff: 0.030930042266845703\n",
      "Size reduction (%): -495.95959595959596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_int8, int8_ms = run_and_time(int8_model_path, x)\n",
    "int8_size = int8_model_path.stat().st_size\n",
    "\n",
    "print(\"INT8 model:\", int8_model_path)\n",
    "print(\"Size (bytes):\", int8_size)\n",
    "print(\"Avg latency (ms):\", int8_ms)\n",
    "print(\"y_int8:\", y_int8)\n",
    "\n",
    "print(\"\\nAllclose(fp32, int8):\", np.allclose(y_fp32, y_int8, rtol=1e-02, atol=1e-03))\n",
    "print(\"Max abs diff:\", float(np.max(np.abs(y_fp32 - y_int8))))\n",
    "print(\"Size reduction (%):\", (1 - int8_size / fp32_size) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c4fe5",
   "metadata": {},
   "source": [
    "## Verificación (checklist)\n",
    "- `outputs/m3_linear_int8/` contiene al menos un `.onnx`.\n",
    "- El modelo INT8 carga con ORT (sin errores).\n",
    "- `np.allclose(...)` es `True` (o el error es pequeño).\n",
    "- (Opcional) el tamaño del archivo baja y/o la latencia mejora.\n",
    "\n",
    "## Siguiente paso (M4)\n",
    "Medición más seria (latencia/throughput), empaquetado y preparación para hardware objetivo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
