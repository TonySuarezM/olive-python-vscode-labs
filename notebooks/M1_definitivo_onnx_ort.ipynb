{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6477c83f",
   "metadata": {},
   "source": [
    "# M1 — Fundamentos: ONNX + ONNX Runtime (y dónde encaja Olive)\n",
    "\n",
    "## Objetivo\n",
    "En este laboratorio vas a:\n",
    "\n",
    "1. Crear un modelo **ONNX** mínimo (un grafo que calcula `Y = X + 1`).\n",
    "2. Guardarlo en `models/`.\n",
    "3. Ejecutarlo con **ONNX Runtime** (ORT) desde Python.\n",
    "4. Inspeccionar **Execution Providers** disponibles y verificar el resultado.\n",
    "\n",
    "## Prerrequisitos\n",
    "- Estar usando el **kernel** correcto en VS Code (tu `.venv`).\n",
    "- Tener instalados: `numpy`, `onnx`, `onnxruntime`.\n",
    "\n",
    "> Nota: si alguna importación falla, usa la celda de instalación (más abajo) y vuelve a ejecutar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57515af0",
   "metadata": {},
   "source": [
    "## 0) Estructura recomendada del proyecto\n",
    "\n",
    "```\n",
    "notebooks/\n",
    "models/\n",
    "outputs/\n",
    "cache/        (opcional, para M2+)\n",
    "requirements.txt (opcional)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d62980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: carpetas creadas/ya existen fuera de notebooks\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Crear carpetas al mismo nivel que notebooks\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"OK: carpetas creadas/ya existen fuera de notebooks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e2338",
   "metadata": {},
   "source": [
    "## 1) Chequeo de entorno (Python, paquetes, providers)\n",
    "\n",
    "Esta celda te ayuda a confirmar que:\n",
    "- El notebook está ejecutándose con el Python del entorno correcto.\n",
    "- Qué versiones tienes instaladas.\n",
    "- Qué providers (EPs) están disponibles en tu ORT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373fe492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "\n",
      "--- pip show onnx ---\n",
      "\n",
      "--- pip show onnxruntime ---\n",
      "\n",
      "--- pip show numpy ---\n",
      "\n",
      "--- pip show olive-ai ---\n",
      "\n",
      "--- pip check ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\source\\\\VisualCode\\\\repos\\\\olive-python-vscode-labs\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'check'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "for pkg in [\"onnx\", \"onnxruntime\", \"numpy\", \"olive-ai\"]:\n",
    "    print(f\"\\n--- pip show {pkg} ---\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", pkg], check=False)\n",
    "\n",
    "print(\"\\n--- pip check ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"check\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348749c0",
   "metadata": {},
   "source": [
    "## 2) (Opcional) Instalar dependencias si faltan\n",
    "\n",
    "Ejecuta esta celda **solo si** en la celda anterior viste errores de importación o paquetes ausentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7445ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\source\\\\VisualCode\\\\repos\\\\olive-python-vscode-labs\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', 'numpy', 'onnx', 'onnxruntime'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "# Si ya lo tienes, pip dirá \"Requirement already satisfied\"\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"numpy\", \"onnx\", \"onnxruntime\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f84fb",
   "metadata": {},
   "source": [
    "## 3) Crear un modelo ONNX mínimo (Y = X + 1)\n",
    "\n",
    "Vamos a construir un grafo ONNX con:\n",
    "- 1 entrada `X` (vector 1D de float32, longitud variable)\n",
    "- 1 constante `C = [1.0]`\n",
    "- 1 nodo `Add(X, C) -> Y`\n",
    "\n",
    "### Compatibilidad (IR / opset)\n",
    "A veces pueden aparecer errores de compatibilidad (por ejemplo, “Unsupported model IR version”).\n",
    "Para evitarlo, fijamos explícitamente:\n",
    "- `ir_version = 11`\n",
    "- `opset = 11`\n",
    "\n",
    "Si tu runtime soporta versiones más altas, esto seguirá funcionando.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfcefc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\models\\add_const.onnx\n",
      "Model ir_version: 11\n",
      "Model opsets: [('', 11)]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "# Ruta de salida (coincide con la celda que crea ../models)\n",
    "model_path = Path(\"../models\") / \"add_const.onnx\"\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# IO\n",
    "X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [None])  # vector 1D tamaño variable\n",
    "Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None])\n",
    "\n",
    "# Constante C = [1.0]\n",
    "C_value = np.array([1.0], dtype=np.float32)\n",
    "C = numpy_helper.from_array(C_value, name=\"C\")\n",
    "\n",
    "# Nodo Add\n",
    "node = helper.make_node(\"Add\", inputs=[\"X\", \"C\"], outputs=[\"Y\"])\n",
    "\n",
    "# Grafo\n",
    "graph = helper.make_graph(\n",
    "    nodes=[node],\n",
    "    name=\"add-const\",\n",
    "    inputs=[X],\n",
    "    outputs=[Y],\n",
    "    initializer=[C],\n",
    ")\n",
    "\n",
    "# Modelo (fijando opset e ir_version para compatibilidad)\n",
    "opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "model = helper.make_model(\n",
    "    graph,\n",
    "    producer_name=\"m1-lab\",\n",
    "    opset_imports=opset,\n",
    "    ir_version=11,\n",
    ")\n",
    "\n",
    "# Validar + guardar\n",
    "onnx.checker.check_model(model)\n",
    "onnx.save_model(model, str(model_path))\n",
    "\n",
    "print(\"Saved:\", model_path)\n",
    "print(\"Model ir_version:\", model.ir_version)\n",
    "print(\"Model opsets:\", [(o.domain, o.version) for o in model.opset_import])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e14a16",
   "metadata": {},
   "source": [
    "## 4) Ejecutar inferencia con ONNX Runtime\n",
    "\n",
    "Cargamos el modelo con `InferenceSession` y ejecutamos con un input `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c6a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORT version: 1.23.2\n",
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "input_name: X | output_name: Y\n",
      "x: [10. 20. 30.]\n",
      "y: [11. 21. 31.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"ORT version:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n",
    "\n",
    "# Cargar modelo y forzar CPU (reproducible)\n",
    "sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "\n",
    "x = np.array([10, 20, 30], dtype=np.float32)\n",
    "y = sess.run([output_name], {input_name: x})[0]\n",
    "\n",
    "print(\"input_name:\", input_name, \"| output_name:\", output_name)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75f983",
   "metadata": {},
   "source": [
    "## 5) Inspección rápida del modelo ONNX (grafo, nodos, inicializadores)\n",
    "\n",
    "Esto te ayuda a “ver” qué contiene el archivo `.onnx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dabc7e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_version: 11\n",
      "opset_imports: [('', 11)]\n",
      "\n",
      "Graph: add-const\n",
      "Inputs: ['X']\n",
      "Outputs: ['Y']\n",
      "Initializers: ['C']\n",
      "\n",
      "Nodes:\n",
      "- Add inputs= ['X', 'C'] outputs= ['Y']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "m = onnx.load(str(model_path))\n",
    "print(\"ir_version:\", m.ir_version)\n",
    "print(\"opset_imports:\", [(o.domain, o.version) for o in m.opset_import])\n",
    "\n",
    "print(\"\\nGraph:\", m.graph.name)\n",
    "print(\"Inputs:\", [i.name for i in m.graph.input])\n",
    "print(\"Outputs:\", [o.name for o in m.graph.output])\n",
    "print(\"Initializers:\", [t.name for t in m.graph.initializer])\n",
    "\n",
    "print(\"\\nNodes:\")\n",
    "for n in m.graph.node:\n",
    "    print(\"-\", n.op_type, \"inputs=\", list(n.input), \"outputs=\", list(n.output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff79b2",
   "metadata": {},
   "source": [
    "## Verificación (checklist)\n",
    "\n",
    "- `models/add_const.onnx` existe.\n",
    "- La inferencia imprime:\n",
    "  - `x: [10. 20. 30.]`\n",
    "  - `y: [11. 21. 31.]`\n",
    "- `Available providers` incluye `CPUExecutionProvider`.\n",
    "\n",
    "Si esto pasa, M1 está completado ✅\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a61a2f",
   "metadata": {},
   "source": [
    "## Errores comunes y diagnóstico\n",
    "\n",
    "### 1) `ModuleNotFoundError`\n",
    "- Asegúrate de estar en el kernel correcto (tu `.venv`).\n",
    "- Ejecuta la celda **2) Instalar dependencias** y reinicia el kernel.\n",
    "\n",
    "### 2) `Unsupported model IR version ...`\n",
    "- Mantén `ir_version=11` y `opset=11` como está en la celda 3.\n",
    "- Si quieres usar versiones más nuevas, actualiza `onnxruntime`.\n",
    "\n",
    "### 3) El modelo corre pero no ves providers GPU/NPU\n",
    "- `ort.get_available_providers()` lista lo que tienes realmente instalado en ese entorno.\n",
    "- Para GPU/NPU normalmente necesitas un paquete de ORT/EP específico para tu hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891e051",
   "metadata": {},
   "source": [
    "## Siguiente paso (M2)\n",
    "\n",
    "En M2 haremos un workflow real de **Olive** (YAML/JSON) para:\n",
    "- tomar un modelo ONNX\n",
    "- aplicar passes de optimización\n",
    "- generar artefactos en `outputs/`\n",
    "- validar antes/después con ORT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
