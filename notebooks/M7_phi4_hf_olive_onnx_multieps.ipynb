{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eb485c",
   "metadata": {},
   "source": [
    "# M7 — Hugging Face → Olive → ONNX → ONNX Runtime (CPU / NVIDIA GPU / Intel NPU)\n",
    "\n",
    "Este notebook integra lo aprendido en M0–M6:\n",
    "- Comprobación de entorno (Python / pip / Olive / ONNX Runtime).\n",
    "- Exportación/optimización con **Olive** en formato **ONNX**.\n",
    "- Ejecución y medición de rendimiento con **ONNX Runtime** usando distintos **Execution Providers**.\n",
    "\n",
    "> Si tu máquina no tiene GPU NVIDIA o NPU Intel (OpenVINO), esas secciones se **saltan** automáticamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f8097",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "1. Descargar el modelo `microsoft/Phi-4-mini-instruct` desde Hugging Face.\n",
    "2. Convertirlo a ONNX y aplicar cuantización INT8 orientada a CPU.\n",
    "3. Probar inferencia y latencia en:\n",
    "   - CPUExecutionProvider\n",
    "   - CUDAExecutionProvider (si está disponible)\n",
    "   - OpenVINOExecutionProvider con `device_type=\"NPU\"` (si está disponible; si falla, fallback a CPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ad7e9",
   "metadata": {},
   "source": [
    "## Referencias oficiales (lista cerrada del curso)\n",
    "\n",
    "- Olive CLI: https://microsoft.github.io/Olive/0.6.1/features/cli.html\n",
    "- Olive Hugging Face: https://microsoft.github.io/Olive/0.7.0/features/huggingface_model_optimization.html\n",
    "- Olive cuantización: https://microsoft.github.io/Olive/features/quantization.html\n",
    "- ORT Execution Providers: https://onnxruntime.ai/docs/execution-providers/\n",
    "- ORT CUDA EP: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html\n",
    "- ORT OpenVINO EP: https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html\n",
    "- ORT Python API: https://onnxruntime.ai/docs/api/python/api_summary.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b525c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Versiones y entorno (OBLIGATORIO antes de comandos de Olive)\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"\\n== pip show olive-ai ==\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n",
    "\n",
    "print(\"\\n== ONNX Runtime ==\")\n",
    "print(\"ORT version:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a20bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2 — Carpetas del proyecto\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../outputs\").mkdir(exist_ok=True)\n",
    "Path(\"../data\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"OK. Carpetas creadas/confirmadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b15bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 — Modelo de Hugging Face\n",
    "MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93365682",
   "metadata": {},
   "source": [
    "## 4) Crear el run-config de Olive\n",
    "\n",
    "Usamos `HfModel` como input y aplicamos:\n",
    "- `OnnxConversion`\n",
    "- `OnnxDynamicQuantization` (dinámica; no requiere dataset de calibración)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "run_dir = Path(\"../outputs\") / \"m7_phi4\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_config_path = run_dir / \"m7_run_config_cpu_int8.json\"\n",
    "cache_dir = run_dir / \"cache\"\n",
    "output_dir = run_dir / \"out_cpu_int8\"\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m7_phi4_cpu_int8\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"HfModel\",\n",
    "        \"model_path\": MODEL_ID,\n",
    "        # No especificamos io_config para permitir que Olive lo infiera automáticamente\n",
    "        # Phi-4 es un modelo CausalLM con KV cache, Olive lo detecta\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"convert_to_onnx\": {\n",
    "            \"type\": \"OnnxConversion\",\n",
    "            \"config\": {\n",
    "                \"target_opset\": 17,  # Opset compatible con modelos modernos\n",
    "            }\n",
    "        },\n",
    "        \"quant_int8_dynamic\": {\"type\": \"OnnxDynamicQuantization\", \"config\": {}},\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", run_config_path)\n",
    "print(\"Output dir:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033251f8",
   "metadata": {},
   "source": [
    "## 5) Ejecutar Olive\n",
    "\n",
    "Usamos `python -m olive run --config ...` para asegurar que se ejecuta con el mismo Python del kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(run_config_path)]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(\"returncode:\", p.returncode)\n",
    "print(\"---- stdout (tail) ----\")\n",
    "print(p.stdout[-4000:])\n",
    "print(\"---- stderr (tail) ----\")\n",
    "print(p.stderr[-4000:])\n",
    "\n",
    "if p.returncode != 0:\n",
    "    raise RuntimeError(\"Olive falló. Revisa el log anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5b73",
   "metadata": {},
   "source": [
    "## 6) Localizar el ONNX resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "onnx_files = sorted(Path(output_dir).rglob(\"*.onnx\"))\n",
    "print(\"Encontrados:\", len(onnx_files))\n",
    "for f in onnx_files[:10]:\n",
    "    print(\"-\", f)\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(\"No se encontraron .onnx en el output_dir. Revisa el log de Olive y la configuración.\")\n",
    "\n",
    "optimized_onnx = onnx_files[0]\n",
    "print(\"\\nUsaremos:\", optimized_onnx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd42558",
   "metadata": {},
   "source": [
    "## 7) Benchmark multi-EP (CPU / CUDA / OpenVINO)\n",
    "\n",
    "- CPU: siempre\n",
    "- CUDA: si `CUDAExecutionProvider` aparece en `ort.get_available_providers()`\n",
    "- OpenVINO: si `OpenVINOExecutionProvider` aparece; intenta `device_type=\"NPU\"`, si falla hace fallback a `CPU`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "model_path = str(optimized_onnx)\n",
    "\n",
    "def make_dummy_inputs(session: ort.InferenceSession, seq_len: int = 8, batch: int = 1):\n",
    "    inputs = {}\n",
    "    for inp in session.get_inputs():\n",
    "        name = inp.name\n",
    "        shape = []\n",
    "        for d in inp.shape:\n",
    "            if d is None:\n",
    "                shape.append(batch if len(shape) == 0 else seq_len)\n",
    "            else:\n",
    "                shape.append(d)\n",
    "\n",
    "        t = inp.type\n",
    "        if \"int64\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.int64)\n",
    "        elif \"int32\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.int32)\n",
    "        elif \"float16\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.float16)\n",
    "        else:\n",
    "            arr = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if name.lower() in [\"input_ids\", \"input\", \"ids\"]:\n",
    "            arr = np.random.randint(0, 1000, size=shape, dtype=arr.dtype)\n",
    "        elif name.lower() in [\"attention_mask\", \"mask\"]:\n",
    "            arr = np.ones(shape, dtype=arr.dtype)\n",
    "\n",
    "        inputs[name] = arr\n",
    "    return inputs\n",
    "\n",
    "def benchmark_session(session: ort.InferenceSession, inputs: dict, warmup: int = 2, iters: int = 5):\n",
    "    for _ in range(warmup):\n",
    "        session.run(None, inputs)\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        session.run(None, inputs)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) * 1000.0 / iters\n",
    "\n",
    "results = []\n",
    "\n",
    "# CPU\n",
    "sess_cpu = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "lat_cpu = benchmark_session(sess_cpu, make_dummy_inputs(sess_cpu))\n",
    "results.append((\"CPUExecutionProvider\", lat_cpu))\n",
    "\n",
    "# CUDA\n",
    "if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "    try:\n",
    "        import onnxruntime\n",
    "        if hasattr(onnxruntime, \"preload_dlls\"):\n",
    "            onnxruntime.preload_dlls()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    sess_cuda = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "    lat_cuda = benchmark_session(sess_cuda, make_dummy_inputs(sess_cuda), warmup=2, iters=10)\n",
    "    results.append((\"CUDAExecutionProvider\", lat_cuda))\n",
    "else:\n",
    "    print(\"CUDAExecutionProvider no disponible (¿onnxruntime-gpu instalado?).\")\n",
    "\n",
    "# OpenVINO\n",
    "if \"OpenVINOExecutionProvider\" in ort.get_available_providers():\n",
    "    def try_openvino(device_type: str):\n",
    "        sess = ort.InferenceSession(model_path, providers=[(\"OpenVINOExecutionProvider\", {\"device_type\": device_type})])\n",
    "        lat = benchmark_session(sess, make_dummy_inputs(sess), warmup=2, iters=10)\n",
    "        return lat\n",
    "\n",
    "    try:\n",
    "        results.append((\"OpenVINOExecutionProvider (NPU)\", try_openvino(\"NPU\")))\n",
    "    except Exception as e:\n",
    "        print(\"OpenVINO NPU falló; fallback a CPU. Error:\", e)\n",
    "        results.append((\"OpenVINOExecutionProvider (CPU)\", try_openvino(\"CPU\")))\n",
    "else:\n",
    "    print(\"OpenVINOExecutionProvider no disponible (¿onnxruntime-openvino instalado + setupvars?).\")\n",
    "\n",
    "print(\"\\nResultados (ms/iter):\")\n",
    "for name, ms in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"- {name}: {ms:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69da55",
   "metadata": {},
   "source": [
    "## Verificación\n",
    "\n",
    "- Olive genera al menos un `.onnx` en `outputs/m7_phi4/out_cpu_int8/`.\n",
    "- CPU benchmark funciona (siempre).\n",
    "- CUDA/OpenVINO solo si aparecen en `ort.get_available_providers()`.\n",
    "\n",
    "## Errores comunes\n",
    "\n",
    "- Kernel equivocado en VS Code → revisa `sys.executable` y elige kernel.\n",
    "- CUDA EP no aparece → necesitas build con CUDA EP (ver doc del CUDA EP).\n",
    "- OpenVINO EP no aparece o NPU falla → `onnxruntime-openvino` + `setupvars.bat` y revisar compatibilidad (ver doc del OpenVINO EP).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
