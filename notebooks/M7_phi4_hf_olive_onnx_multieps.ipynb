{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eb485c",
   "metadata": {},
   "source": [
    "# M7 — Hugging Face → Olive → ONNX → ONNX Runtime (CPU / NVIDIA GPU / Intel NPU)\n",
    "\n",
    "Este notebook integra lo aprendido en M0–M6:\n",
    "- Comprobación de entorno (Python / pip / Olive / ONNX Runtime).\n",
    "- Exportación/optimización con **Olive** en formato **ONNX**.\n",
    "- Ejecución y medición de rendimiento con **ONNX Runtime** usando distintos **Execution Providers**.\n",
    "\n",
    "> Si tu máquina no tiene GPU NVIDIA o NPU Intel (OpenVINO), esas secciones se **saltan** automáticamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f8097",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "1. Descargar el modelo `microsoft/Phi-4-mini-instruct` desde Hugging Face.\n",
    "2. Convertirlo a ONNX y aplicar cuantización INT8 orientada a CPU.\n",
    "3. Probar inferencia y latencia en:\n",
    "   - CPUExecutionProvider\n",
    "   - CUDAExecutionProvider (si está disponible)\n",
    "   - OpenVINOExecutionProvider con `device_type=\"NPU\"` (si está disponible; si falla, fallback a CPU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ad7e9",
   "metadata": {},
   "source": [
    "## Referencias oficiales (lista cerrada del curso)\n",
    "\n",
    "- Olive CLI: https://microsoft.github.io/Olive/0.6.1/features/cli.html\n",
    "- Olive Hugging Face: https://microsoft.github.io/Olive/0.7.0/features/huggingface_model_optimization.html\n",
    "- Olive cuantización: https://microsoft.github.io/Olive/features/quantization.html\n",
    "- ORT Execution Providers: https://onnxruntime.ai/docs/execution-providers/\n",
    "- ORT CUDA EP: https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html\n",
    "- ORT OpenVINO EP: https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html\n",
    "- ORT Python API: https://onnxruntime.ai/docs/api/python/api_summary.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b525c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe\n",
      "Python version: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "\n",
      "== pip show olive-ai ==\n",
      "\n",
      "== ONNX Runtime ==\n",
      "ORT version: 1.23.2\n",
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# Celda 1 — Versiones y entorno (OBLIGATORIO antes de comandos de Olive)\n",
    "import sys, subprocess\n",
    "import onnxruntime as ort\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "print(\"\\n== pip show olive-ai ==\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n",
    "\n",
    "print(\"\\n== ONNX Runtime ==\")\n",
    "print(\"ORT version:\", ort.get_version_string())\n",
    "print(\"Available providers:\", ort.get_available_providers())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a20bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Carpetas creadas/confirmadas.\n"
     ]
    }
   ],
   "source": [
    "# Celda 2 — Carpetas del proyecto\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"../models\").mkdir(exist_ok=True)\n",
    "Path(\"../outputs\").mkdir(exist_ok=True)\n",
    "Path(\"../data\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"OK. Carpetas creadas/confirmadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b15bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_ID: microsoft/Phi-4-mini-instruct\n"
     ]
    }
   ],
   "source": [
    "# Celda 3 — Modelo de Hugging Face\n",
    "MODEL_ID = \"microsoft/Phi-4-mini-instruct\"\n",
    "print(\"MODEL_ID:\", MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93365682",
   "metadata": {},
   "source": [
    "## 4) Crear el run-config de Olive\n",
    "\n",
    "Usamos `HfModel` como input y aplicamos:\n",
    "- `OnnxConversion`\n",
    "- `OnnxDynamicQuantization` (dinámica; no requiere dataset de calibración)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bce2aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ..\\outputs\\m7_phi4\\m7_run_config_cpu_int8.json\n",
      "Output dir: ..\\outputs\\m7_phi4\\out_cpu_int8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "run_dir = Path(\"../outputs\") / \"m7_phi4\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_config_path = run_dir / \"m7_run_config_cpu_int8.json\"\n",
    "cache_dir = run_dir / \"cache\"\n",
    "output_dir = run_dir / \"out_cpu_int8\"\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m7_phi4_cpu_int8\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"HfModel\",\n",
    "        \"model_path\": MODEL_ID,\n",
    "        # No especificamos io_config para permitir que Olive lo infiera automáticamente\n",
    "        # Phi-4 es un modelo CausalLM con KV cache, Olive lo detecta\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"convert_to_onnx\": {\n",
    "            \"type\": \"OnnxConversion\",\n",
    "            \"config\": {\n",
    "                \"target_opset\": 17,  # Opset compatible con modelos modernos\n",
    "            }\n",
    "        },\n",
    "        \"quant_int8_dynamic\": {\"type\": \"OnnxDynamicQuantization\", \"config\": {}},\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", run_config_path)\n",
    "print(\"Output dir:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033251f8",
   "metadata": {},
   "source": [
    "## 5) Ejecutar Olive\n",
    "\n",
    "Usamos `python -m olive run --config ...` para asegurar que se ejecuta con el mismo Python del kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bec8948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Scripts\\python.exe -m olive run --config ..\\outputs\\m7_phi4\\m7_run_config_cpu_int8.json\n",
      "returncode: 1\n",
      "---- stdout (tail) ----\n",
      "[2026-01-12 20:20:16,701] [INFO] [run.py:99:run_engine] Running workflow m7_phi4_cpu_int8\n",
      "[2026-01-12 20:20:16,889] [INFO] [cache.py:138:__init__] Using cache directory: C:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m7_phi4\\cache\\m7_phi4_cpu_int8\n",
      "[2026-01-12 20:20:16,892] [DEBUG] [cache.py:274:cache_olive_config] Cached olive config to C:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m7_phi4\\cache\\m7_phi4_cpu_int8\\olive_config.json\n",
      "[2026-01-12 20:20:16,896] [DEBUG] [accelerator_creator.py:106:_fill_accelerators] The accelerator device and execution providers are specified, skipping deduce.\n",
      "[2026-01-12 20:20:16,896] [DEBUG] [accelerator_creator.py:143:_check_execution_providers] Supported execution providers for device cpu: [<ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>, <ExecutionProvider.CPUExecutionProvider: 'CPUExecutionProvider'>]\n",
      "[2026-01-12 20:20:16,896] [DEBUG] [accelerator_creator.py:179:create_accelerator] Initial accelerators and execution providers: {'cpu': ['CPUExecutionProvider']}\n",
      "[2026-01-12 20:20:16,896] [INFO] [accelerator_creator.py:195:create_accelerator] Running workflow on accelerator spec: cpu-cpu\n",
      "[2026-01-12 20:20:16,896] [DEBUG] [cache.py:295:set_cache_env] Set OLIVE_CACHE_DIR: C:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\outputs\\m7_phi4\\cache\\m7_phi4_cpu_int8\n",
      "[2026-01-12 20:20:16,897] [INFO] [engine.py:206:run] Running Olive on accelerator: cpu-cpu\n",
      "[2026-01-12 20:20:16,897] [INFO] [engine.py:824:_create_system] Creating target system ...\n",
      "[2026-01-12 20:20:16,897] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2026-01-12 20:20:16,897] [INFO] [engine.py:827:_create_system] Target system created in 0.000498 seconds\n",
      "[2026-01-12 20:20:16,897] [INFO] [engine.py:830:_create_system] Creating host system ...\n",
      "[2026-01-12 20:20:16,898] [DEBUG] [engine.py:820:create_system] create native OliveSystem LocalSystem\n",
      "[2026-01-12 20:20:16,898] [INFO] [engine.py:833:_create_system] Host system created in 0.000379 seconds\n",
      "[2026-01-12 20:20:17,478] [DEBUG] [engine.py:284:run_accelerator] Running Olive in no-search mode ...\n",
      "[2026-01-12 20:20:17,604] [DEBUG] [engine.py:326:_run_no_search] Running ['convert_to_onnx', 'quant_int8_dynamic'] with no search ...\n",
      "[2026-01-12 20:20:17,604] [INFO] [engine.py:656:_run_pass] Running pass convert_to_onnx:onnxconversion\n",
      "[2026-01-12 20:20:21,628] [DEBUG] [utils.py:63:load_model_from_task] Loaded model <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'> with name_or_path microsoft/Phi-4-mini-instruct\n",
      "[2026-01-12 20:20:21,630] [DEBUG] [hf.py:120:get_dummy_inputs] Trying hf optimum export config to get dummy inputs\n",
      "[2026-01-12 20:20:21,632] [DEBUG] [model_io.py:51:get_export_config] optimum is not installed. Cannot get export config\n",
      "\n",
      "---- stderr (tail) ----\n",
      "r_spec,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\engine\\engine.py\"\u001b[0m, line \u001b[35m285\u001b[0m, in \u001b[35mrun_accelerator\u001b[0m\n",
      "    \u001b[31mself._run_no_search\u001b[0m\u001b[1;31m(input_model_config, input_model_id, accelerator_spec, artifacts_dir)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\engine\\engine.py\"\u001b[0m, line \u001b[35m327\u001b[0m, in \u001b[35m_run_no_search\u001b[0m\n",
      "    should_prune, signal, model_ids = \u001b[31mself._run_passes\u001b[0m\u001b[1;31m(input_model_config, input_model_id, accelerator_spec)\u001b[0m\n",
      "                                      \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\engine\\engine.py\"\u001b[0m, line \u001b[35m612\u001b[0m, in \u001b[35m_run_passes\u001b[0m\n",
      "    model_config, model_id = \u001b[31mself._run_pass\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                             \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mpass_name,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31maccelerator_spec,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\engine\\engine.py\"\u001b[0m, line \u001b[35m707\u001b[0m, in \u001b[35m_run_pass\u001b[0m\n",
      "    output_model_config = host.run_pass(p, input_model_config, output_model_path)\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\systems\\local.py\"\u001b[0m, line \u001b[35m45\u001b[0m, in \u001b[35mrun_pass\u001b[0m\n",
      "    output_model = the_pass.run(model, output_model_path)\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\passes\\olive_pass.py\"\u001b[0m, line \u001b[35m242\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    output_model = self._run_for_config(model, self.config, output_model_path)\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\passes\\onnx\\conversion.py\"\u001b[0m, line \u001b[35m502\u001b[0m, in \u001b[35m_run_for_config\u001b[0m\n",
      "    output_model = self._run_for_config_internal(model, config, output_model_path)\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\passes\\onnx\\conversion.py\"\u001b[0m, line \u001b[35m542\u001b[0m, in \u001b[35m_run_for_config_internal\u001b[0m\n",
      "    return \u001b[31mself._convert_model_on_device\u001b[0m\u001b[1;31m(model, config, output_model_path, device, torch_dtype)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\passes\\onnx\\conversion.py\"\u001b[0m, line \u001b[35m564\u001b[0m, in \u001b[35m_convert_model_on_device\u001b[0m\n",
      "    dummy_inputs = _get_dummy_inputs(model, config)\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\passes\\onnx\\conversion.py\"\u001b[0m, line \u001b[35m298\u001b[0m, in \u001b[35m_get_dummy_inputs\u001b[0m\n",
      "    return \u001b[31mmodel.get_dummy_inputs\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mfilter_hook=(model.merge_kv_cache_hook if config.use_dynamo_exporter else model.merge_kv_cache_to_tuple_hook),\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31m},\u001b[0m\n",
      "        \u001b[1;31m^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"c:\\source\\VisualCode\\repos\\olive-python-vscode-labs\\.venv\\Lib\\site-packages\\olive\\model\\handler\\hf.py\"\u001b[0m, line \u001b[35m125\u001b[0m, in \u001b[35mget_dummy_inputs\u001b[0m\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "\u001b[1;35mValueError\u001b[0m: \u001b[35mUnable to get dummy inputs for the model. Please provide io_config or install an optimum version that supports the model for export.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Olive falló. Revisa el log anterior.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(p.stderr[-\u001b[32m4000\u001b[39m:])\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOlive falló. Revisa el log anterior.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Olive falló. Revisa el log anterior."
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(run_config_path)]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "\n",
    "p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print(\"returncode:\", p.returncode)\n",
    "print(\"---- stdout (tail) ----\")\n",
    "print(p.stdout[-4000:])\n",
    "print(\"---- stderr (tail) ----\")\n",
    "print(p.stderr[-4000:])\n",
    "\n",
    "if p.returncode != 0:\n",
    "    raise RuntimeError(\"Olive falló. Revisa el log anterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba5b73",
   "metadata": {},
   "source": [
    "## 6) Localizar el ONNX resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "onnx_files = sorted(Path(output_dir).rglob(\"*.onnx\"))\n",
    "print(\"Encontrados:\", len(onnx_files))\n",
    "for f in onnx_files[:10]:\n",
    "    print(\"-\", f)\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(\"No se encontraron .onnx en el output_dir. Revisa el log de Olive y la configuración.\")\n",
    "\n",
    "optimized_onnx = onnx_files[0]\n",
    "print(\"\\nUsaremos:\", optimized_onnx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd42558",
   "metadata": {},
   "source": [
    "## 7) Benchmark multi-EP (CPU / CUDA / OpenVINO)\n",
    "\n",
    "- CPU: siempre\n",
    "- CUDA: si `CUDAExecutionProvider` aparece en `ort.get_available_providers()`\n",
    "- OpenVINO: si `OpenVINOExecutionProvider` aparece; intenta `device_type=\"NPU\"`, si falla hace fallback a `CPU`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "model_path = str(optimized_onnx)\n",
    "\n",
    "def make_dummy_inputs(session: ort.InferenceSession, seq_len: int = 8, batch: int = 1):\n",
    "    inputs = {}\n",
    "    for inp in session.get_inputs():\n",
    "        name = inp.name\n",
    "        shape = []\n",
    "        for d in inp.shape:\n",
    "            if d is None:\n",
    "                shape.append(batch if len(shape) == 0 else seq_len)\n",
    "            else:\n",
    "                shape.append(d)\n",
    "\n",
    "        t = inp.type\n",
    "        if \"int64\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.int64)\n",
    "        elif \"int32\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.int32)\n",
    "        elif \"float16\" in t:\n",
    "            arr = np.zeros(shape, dtype=np.float16)\n",
    "        else:\n",
    "            arr = np.zeros(shape, dtype=np.float32)\n",
    "\n",
    "        if name.lower() in [\"input_ids\", \"input\", \"ids\"]:\n",
    "            arr = np.random.randint(0, 1000, size=shape, dtype=arr.dtype)\n",
    "        elif name.lower() in [\"attention_mask\", \"mask\"]:\n",
    "            arr = np.ones(shape, dtype=arr.dtype)\n",
    "\n",
    "        inputs[name] = arr\n",
    "    return inputs\n",
    "\n",
    "def benchmark_session(session: ort.InferenceSession, inputs: dict, warmup: int = 2, iters: int = 5):\n",
    "    for _ in range(warmup):\n",
    "        session.run(None, inputs)\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        session.run(None, inputs)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) * 1000.0 / iters\n",
    "\n",
    "results = []\n",
    "\n",
    "# CPU\n",
    "sess_cpu = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "lat_cpu = benchmark_session(sess_cpu, make_dummy_inputs(sess_cpu))\n",
    "results.append((\"CPUExecutionProvider\", lat_cpu))\n",
    "\n",
    "# CUDA\n",
    "if \"CUDAExecutionProvider\" in ort.get_available_providers():\n",
    "    try:\n",
    "        import onnxruntime\n",
    "        if hasattr(onnxruntime, \"preload_dlls\"):\n",
    "            onnxruntime.preload_dlls()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    sess_cuda = ort.InferenceSession(model_path, providers=[\"CUDAExecutionProvider\"])\n",
    "    lat_cuda = benchmark_session(sess_cuda, make_dummy_inputs(sess_cuda), warmup=2, iters=10)\n",
    "    results.append((\"CUDAExecutionProvider\", lat_cuda))\n",
    "else:\n",
    "    print(\"CUDAExecutionProvider no disponible (¿onnxruntime-gpu instalado?).\")\n",
    "\n",
    "# OpenVINO\n",
    "if \"OpenVINOExecutionProvider\" in ort.get_available_providers():\n",
    "    def try_openvino(device_type: str):\n",
    "        sess = ort.InferenceSession(model_path, providers=[(\"OpenVINOExecutionProvider\", {\"device_type\": device_type})])\n",
    "        lat = benchmark_session(sess, make_dummy_inputs(sess), warmup=2, iters=10)\n",
    "        return lat\n",
    "\n",
    "    try:\n",
    "        results.append((\"OpenVINOExecutionProvider (NPU)\", try_openvino(\"NPU\")))\n",
    "    except Exception as e:\n",
    "        print(\"OpenVINO NPU falló; fallback a CPU. Error:\", e)\n",
    "        results.append((\"OpenVINOExecutionProvider (CPU)\", try_openvino(\"CPU\")))\n",
    "else:\n",
    "    print(\"OpenVINOExecutionProvider no disponible (¿onnxruntime-openvino instalado + setupvars?).\")\n",
    "\n",
    "print(\"\\nResultados (ms/iter):\")\n",
    "for name, ms in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"- {name}: {ms:.3f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69da55",
   "metadata": {},
   "source": [
    "## Verificación\n",
    "\n",
    "- Olive genera al menos un `.onnx` en `outputs/m7_phi4/out_cpu_int8/`.\n",
    "- CPU benchmark funciona (siempre).\n",
    "- CUDA/OpenVINO solo si aparecen en `ort.get_available_providers()`.\n",
    "\n",
    "## Errores comunes\n",
    "\n",
    "- Kernel equivocado en VS Code → revisa `sys.executable` y elige kernel.\n",
    "- CUDA EP no aparece → necesitas build con CUDA EP (ver doc del CUDA EP).\n",
    "- OpenVINO EP no aparece o NPU falla → `onnxruntime-openvino` + `setupvars.bat` y revisar compatibilidad (ver doc del OpenVINO EP).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
