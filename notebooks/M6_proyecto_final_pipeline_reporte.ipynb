{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3249b0fe",
   "metadata": {},
   "source": [
    "# M6 — Proyecto final: pipeline completo + métricas + mini‑reporte (VS Code + Jupyter + Olive + ONNX Runtime)\n",
    "\n",
    "## Objetivo\n",
    "Construir un **pipeline reproducible** que:\n",
    "1. Tome un modelo **ONNX baseline** (nuestro `add_const.onnx` del curso).\n",
    "2. Ejecute un workflow de **Olive** (cuantización dinámica INT8 en CPU).\n",
    "3. Haga una **optimización offline** adicional con ONNX Runtime (serializando el grafo optimizado).\n",
    "4. Mida **tamaño** y **latencia** (benchmark simple) y valide **salida**.\n",
    "5. Genere un **reporte** en `outputs/m6_report.md`.\n",
    "\n",
    "## Prerrequisitos\n",
    "- Ejecutar este notebook con el **kernel correcto** (tu `.venv` del proyecto) en VS Code.\n",
    "- Tener instalado:\n",
    "  - `onnxruntime`\n",
    "  - `onnx`\n",
    "  - `numpy`\n",
    "  - `olive-ai`\n",
    "\n",
    "> Si estás en Windows y tienes varios Pythons instalados, comprueba `sys.executable` en la celda 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1 — Comprobación de entorno (kernel, venv y versiones)\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# Heurística de venv: sys.prefix != sys.base_prefix\n",
    "print(\"sys.prefix:\", sys.prefix)\n",
    "print(\"sys.base_prefix:\", sys.base_prefix)\n",
    "print(\"Running in venv?:\", sys.prefix != sys.base_prefix)\n",
    "\n",
    "print(\"\\n--- pip show olive-ai ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"olive-ai\"], check=False)\n",
    "\n",
    "print(\"\\n--- pip show onnxruntime ---\")\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", \"onnxruntime\"], check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42932058",
   "metadata": {},
   "source": [
    "## Estructura del proyecto (recomendada)\n",
    "Este notebook asume la estructura:\n",
    "\n",
    "- `models/`  → modelos ONNX baseline\n",
    "- `outputs/` → salidas de workflows y reportes\n",
    "\n",
    "Si tu notebook está en `notebooks/`, detectamos la raíz del repo automáticamente (celda 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ce4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2 — Resolver rutas del proyecto (root/models/outputs)\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd()\n",
    "if not (root / \"models\").exists() and (root.parent / \"models\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "models_dir = root / \"models\"\n",
    "outputs_dir = root / \"outputs\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", root)\n",
    "print(\"models_dir:\", models_dir)\n",
    "print(\"outputs_dir:\", outputs_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8a2a4",
   "metadata": {},
   "source": [
    "## 0) Baseline: asegurar que existe `models/add_const.onnx`\n",
    "\n",
    "Si vienes de M1, ya lo tienes.\n",
    "Si no, esta celda lo genera (Add(X, C) -> Y con C=1.0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3 — (Re)crear baseline ONNX si falta\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import TensorProto, helper, numpy_helper\n",
    "\n",
    "baseline_path = models_dir / \"add_const.onnx\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "    print(\"OK baseline exists:\", baseline_path)\n",
    "else:\n",
    "    # 1) IO\n",
    "    X = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [None])\n",
    "    Y = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [None])\n",
    "\n",
    "    # 2) initializer C\n",
    "    C_value = np.array([1.0], dtype=np.float32)\n",
    "    C = numpy_helper.from_array(C_value, name=\"C\")\n",
    "\n",
    "    # 3) Add\n",
    "    node = helper.make_node(\"Add\", inputs=[\"X\", \"C\"], outputs=[\"Y\"])\n",
    "\n",
    "    # 4) graph + model\n",
    "    graph = helper.make_graph([node], \"add-const\", [X], [Y], initializer=[C])\n",
    "    opset = [helper.make_operatorsetid(\"\", 11)]\n",
    "    model = helper.make_model(graph, producer_name=\"m6-final\", opset_imports=opset)\n",
    "    model.ir_version = 11  # compatibilidad con runtimes antiguos si aplica\n",
    "\n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save_model(model, str(baseline_path))\n",
    "    print(\"Wrote baseline:\", baseline_path)\n",
    "\n",
    "baseline_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65778fa",
   "metadata": {},
   "source": [
    "## 1) Métricas baseline (tamaño, latencia simple, verificación numérica)\n",
    "\n",
    "- **Tamaño**: bytes del archivo ONNX.\n",
    "- **Latencia**: medimos `session.run` en un loop (warmup + runs).\n",
    "- **Verificación**: comparamos la salida con una referencia conocida para un input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a52acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4 — Helpers de medición ORT (CPU)\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def file_size_bytes(p):\n",
    "    return int(Path(p).stat().st_size)\n",
    "\n",
    "def ort_infer_once(model_path, x):\n",
    "    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "    y = sess.run([output_name], {input_name: x})[0]\n",
    "    return y\n",
    "\n",
    "def ort_latency_ms(model_path, x, warmup=20, runs=200):\n",
    "    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    output_name = sess.get_outputs()[0].name\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        sess.run([output_name], {input_name: x})\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(runs):\n",
    "        sess.run([output_name], {input_name: x})\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    return (t1 - t0) * 1000.0 / runs\n",
    "\n",
    "x = np.array([10, 20, 30], dtype=np.float32)\n",
    "\n",
    "baseline_size = file_size_bytes(baseline_path)\n",
    "baseline_y = ort_infer_once(baseline_path, x)\n",
    "baseline_lat_ms = ort_latency_ms(baseline_path, x)\n",
    "\n",
    "print(\"ORT version:\", ort.get_version_string())\n",
    "print(\"Baseline model:\", baseline_path)\n",
    "print(\"Baseline size (bytes):\", baseline_size)\n",
    "print(\"Baseline latency avg (ms):\", baseline_lat_ms)\n",
    "print(\"Baseline y:\", baseline_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a707e123",
   "metadata": {},
   "source": [
    "## 2) Workflow Olive (cuantización dinámica INT8 en CPU)\n",
    "\n",
    "Ejecutamos `python -m olive run --config ...` desde el **mismo Python del kernel**.\n",
    "\n",
    "Salida:\n",
    "- `outputs/m6_workflow/` (output_dir)\n",
    "- `outputs/m6_cache/` (cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b544c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5 — Escribir run config (Olive) para cuantización dinámica (INT8) en CPU\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "run_config_path = outputs_dir / \"m6_run_config.json\"\n",
    "output_dir = outputs_dir / \"m6_workflow\"\n",
    "cache_dir = outputs_dir / \"m6_cache\"\n",
    "\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"workflow_id\": \"m6_final_project_int8_cpu\",\n",
    "    \"input_model\": {\n",
    "        \"type\": \"ONNXModel\",\n",
    "        \"config\": {\"model_path\": str(baseline_path)},\n",
    "    },\n",
    "    \"systems\": {\n",
    "        \"local_system\": {\n",
    "            \"type\": \"LocalSystem\",\n",
    "            \"config\": {\n",
    "                \"accelerators\": [\n",
    "                    {\"device\": \"cpu\", \"execution_providers\": [\"CPUExecutionProvider\"]}\n",
    "                ]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"passes\": {\n",
    "        \"opset_to_11\": {\"type\": \"OnnxOpVersionConversion\", \"config\": {\"target_opset\": 11}},\n",
    "        \"dynamic_int8\": {\n",
    "            \"type\": \"OnnxDynamicQuantization\",\n",
    "            \"config\": {\"quant_mode\": \"dynamic\", \"weight_type\": \"QInt8\"},\n",
    "        },\n",
    "    },\n",
    "    \"engine\": {\n",
    "        \"host\": \"local_system\",\n",
    "        \"target\": \"local_system\",\n",
    "        \"cache_dir\": str(cache_dir),\n",
    "        \"output_dir\": str(output_dir),\n",
    "        \"log_severity_level\": 0,\n",
    "        \"evaluate_input_model\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_config_path.write_text(json.dumps(config, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", run_config_path)\n",
    "print(\"Output dir:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6 — Ejecutar Olive (usa el Python del kernel)\n",
    "import sys, subprocess, shlex\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"olive\", \"run\", \"--config\", str(run_config_path)]\n",
    "print(\"Running:\", \" \".join(shlex.quote(c) for c in cmd))\n",
    "\n",
    "p = subprocess.run(cmd, text=True, capture_output=True)\n",
    "print(\"returncode:\", p.returncode)\n",
    "print(\"\\n--- stdout (first 2000 chars) ---\\n\", p.stdout[:2000])\n",
    "print(\"\\n--- stderr (first 2000 chars) ---\\n\", p.stderr[:2000])\n",
    "\n",
    "if p.returncode != 0:\n",
    "    raise RuntimeError(\"Olive run falló. Revisa stdout/stderr arriba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7493f6",
   "metadata": {},
   "source": [
    "## 3) Localizar el modelo cuantizado generado por Olive\n",
    "\n",
    "Buscamos recursivamente el `.onnx` más reciente dentro de `outputs/m6_workflow/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ed7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7 — Buscar el ONNX output más reciente\n",
    "onnx_files = sorted(output_dir.rglob(\"*.onnx\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "print(\"Found .onnx:\", len(onnx_files))\n",
    "\n",
    "if not onnx_files:\n",
    "    raise FileNotFoundError(f\"No se encontraron .onnx en {output_dir}. Revisa el log de Olive (celda 6).\")\n",
    "\n",
    "quant_path = onnx_files[0]\n",
    "print(\"Selected quantized model:\", quant_path)\n",
    "quant_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8e9c7",
   "metadata": {},
   "source": [
    "## 4) Métricas del modelo cuantizado (tamaño, latencia, verificación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276916d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8 — Métricas del modelo cuantizado\n",
    "quant_size = file_size_bytes(quant_path)\n",
    "quant_y = ort_infer_once(quant_path, x)\n",
    "quant_lat_ms = ort_latency_ms(quant_path, x)\n",
    "\n",
    "abs_diff = float(np.max(np.abs(quant_y - baseline_y)))\n",
    "\n",
    "print(\"Quant model:\", quant_path)\n",
    "print(\"Quant size (bytes):\", quant_size)\n",
    "print(\"Quant latency avg (ms):\", quant_lat_ms)\n",
    "print(\"Quant y:\", quant_y)\n",
    "print(\"Max abs diff vs baseline:\", abs_diff)\n",
    "\n",
    "print(\"\\nSize reduction (%):\", (baseline_size - quant_size) * 100.0 / baseline_size)\n",
    "print(\"Latency improvement (%):\", (baseline_lat_ms - quant_lat_ms) * 100.0 / baseline_lat_ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f53002",
   "metadata": {},
   "source": [
    "## 5) Optimización OFFLINE adicional con ONNX Runtime (serializar modelo optimizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7722227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9 — ORT offline optimization: serializar un ONNX optimizado a disco\n",
    "import onnxruntime as rt\n",
    "\n",
    "final_dir = outputs_dir / \"m6_final\"\n",
    "final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "final_optimized_path = final_dir / \"add_const_int8_ort_optimized.onnx\"\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_options.optimized_model_filepath = str(final_optimized_path)\n",
    "\n",
    "_ = rt.InferenceSession(str(quant_path), sess_options, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Wrote optimized model:\", final_optimized_path)\n",
    "final_optimized_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c11a6",
   "metadata": {},
   "source": [
    "## 6) Métricas del modelo FINAL (cuantizado + ORT offline optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 10 — Métricas del modelo final\n",
    "final_size = file_size_bytes(final_optimized_path)\n",
    "final_y = ort_infer_once(final_optimized_path, x)\n",
    "final_lat_ms = ort_latency_ms(final_optimized_path, x)\n",
    "final_abs_diff = float(np.max(np.abs(final_y - baseline_y)))\n",
    "\n",
    "print(\"Final model:\", final_optimized_path)\n",
    "print(\"Final size (bytes):\", final_size)\n",
    "print(\"Final latency avg (ms):\", final_lat_ms)\n",
    "print(\"Final y:\", final_y)\n",
    "print(\"Max abs diff vs baseline:\", final_abs_diff)\n",
    "\n",
    "print(\"\\nSize reduction vs baseline (%):\", (baseline_size - final_size) * 100.0 / baseline_size)\n",
    "print(\"Latency improvement vs baseline (%):\", (baseline_lat_ms - final_lat_ms) * 100.0 / baseline_lat_ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d47840",
   "metadata": {},
   "source": [
    "## 7) Generar reporte (Markdown)\n",
    "\n",
    "Creamos `outputs/m6_report.md` con tabla comparativa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf17063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 11 — Generar un reporte Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "report_path = outputs_dir / \"m6_report.md\"\n",
    "\n",
    "def fmt_bytes(n):\n",
    "    n = float(n)\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:\n",
    "        if n < 1024.0:\n",
    "            return f\"{n:.1f} {unit}\"\n",
    "        n /= 1024.0\n",
    "    return f\"{n:.1f} TB\"\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"# M6 Reporte — {datetime.now().isoformat(timespec='seconds')}\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Artefactos\")\n",
    "lines.append(f\"- Baseline: `{baseline_path}`\")\n",
    "lines.append(f\"- Cuantizado (Olive): `{quant_path}`\")\n",
    "lines.append(f\"- Final (ORT offline optimized): `{final_optimized_path}`\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Métricas\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"| Variante | Tamaño | Latencia avg (ms) | Max abs diff vs baseline |\")\n",
    "lines.append(\"|---|---:|---:|---:|\")\n",
    "lines.append(f\"| Baseline | {fmt_bytes(baseline_size)} | {baseline_lat_ms:.6f} | 0.0 |\")\n",
    "lines.append(f\"| Cuantizado (Olive) | {fmt_bytes(quant_size)} | {quant_lat_ms:.6f} | {abs_diff:.6f} |\")\n",
    "lines.append(f\"| Final (ORT opt) | {fmt_bytes(final_size)} | {final_lat_ms:.6f} | {final_abs_diff:.6f} |\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Comparativas (vs baseline)\")\n",
    "lines.append(f\"- Reducción de tamaño (Cuantizado): {(baseline_size-quant_size)*100.0/baseline_size:.2f}%\")\n",
    "lines.append(f\"- Reducción de tamaño (Final): {(baseline_size-final_size)*100.0/baseline_size:.2f}%\")\n",
    "lines.append(f\"- Mejora de latencia (Cuantizado): {(baseline_lat_ms-quant_lat_ms)*100.0/baseline_lat_ms:.2f}%\")\n",
    "lines.append(f\"- Mejora de latencia (Final): {(baseline_lat_ms-final_lat_ms)*100.0/baseline_lat_ms:.2f}%\")\n",
    "lines.append(\"\")\n",
    "lines.append(\"## Nota\")\n",
    "lines.append(\"- La latencia aquí es un micro‑benchmark simple en Python.\")\n",
    "\n",
    "report_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "print(\"Wrote report:\", report_path)\n",
    "report_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89222a0a",
   "metadata": {},
   "source": [
    "## (Opcional) Packaging de artefactos con Olive (Zipfile)\n",
    "\n",
    "Olive puede empaquetar artefactos en un ZIP usando `packaging_config` en la sección `engine`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e29292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 12 — (Opcional) preparar un run config con packaging_config (NO ejecuta)\n",
    "packaged_config_path = outputs_dir / \"m6_run_config_packaged.json\"\n",
    "packaged = dict(config)\n",
    "packaged[\"engine\"] = dict(config[\"engine\"])\n",
    "packaged[\"engine\"][\"packaging_config\"] = {\"type\": \"Zipfile\", \"name\": \"M6_OutputModels\"}\n",
    "\n",
    "packaged_config_path.write_text(json.dumps(packaged, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", packaged_config_path)\n",
    "print(\"Para ejecutar:\")\n",
    "print(f\"  {sys.executable} -m olive run --config {packaged_config_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d2b3e",
   "metadata": {},
   "source": [
    "## Verificación (qué debe salir bien)\n",
    "- `outputs/m6_workflow/` contiene salidas del workflow de Olive.\n",
    "- `outputs/m6_final/add_const_int8_ort_optimized.onnx` existe.\n",
    "- `outputs/m6_report.md` existe y resume las métricas.\n",
    "\n",
    "## Errores comunes y diagnóstico\n",
    "1) **Olive se ejecuta con otro Python**\n",
    "   - Síntoma: logs apuntan a otro `...Python...` distinto al `.venv`.\n",
    "   - Solución: usa `sys.executable -m olive ...` y/o selecciona el kernel correcto.\n",
    "\n",
    "2) **No se generan `.onnx` en output_dir**\n",
    "   - Revisa `stdout/stderr` de la celda 6.\n",
    "   - Comprueba que `baseline_path` existe y es válido.\n",
    "\n",
    "3) **ORT falla al cargar el ONNX**\n",
    "   - Puede ser incompatibilidad de IR/opset. Aquí fijamos `opset=11` e `ir_version=11` en el baseline.\n",
    "\n",
    "## Fin del curso\n",
    "Siguiente salto: sustituir el modelo de juguete por uno real (transformer), aplicar passes específicos y definir métricas de calidad/latencia representativas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
